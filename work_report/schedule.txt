starting time:5.9  end time：6.5
Member：Jin,Ziyuan  Cui,Hanjia  Li,Bowei
######################################################################################################
first week
time:5.9-5.11
######################################################################################################
This week's progress:

Jin,Ziyuan:
1.Research on multi-agent reinforcement learning algorithms.
2.Organize the available game AI environment.
3.Sort out the parts that need to be completed in the project.

Cui,Hanjia:
1.Research on multi-agent reinforcement learning algorithms.
2.Consult the thesis materials related to the project.
3.Compile the readme file, the brief introduction of the entire project, etc.

Li,Bowei:
1.Investigate the feasibility and difficulty of some reinforcement learning algorithms.
2.Investigate the feasibility of the research project.
3.Look for recent related working papers.

——————————————————————————————————————————————————————————————————————————————————————————————————————
Plans for Next week:

Jin,Ziyuan:
1.Sort out the project framework and upload the project architecture file.
2.Initially build the code implementation process in each file in blocks.

Cui,Hanjia:
1.Deploy the Unity environment and write a demo to verify the feasibility of the selected environment.
2.Design the interfaces that need to interact with the algorithm in the discussion environment.

Li,Bowei:
1.Write the test program of the PPO algorithm according to the discussion.
2.Verify the feasibility of the algorithm. If it is not feasible, replace it with a new algorithm and 
write a demo for verification.

######################################################################################################
second week
time:5.12-5.18
######################################################################################################
This week's progress:

Jin,Ziyuan:
1. Sort out the project framework and upload the project architecture file.

Cui,Hanjia:
1. Deploy the train environment in the server and make sure it work with basic functionality.
2. Identify the problem of instability and performance variance. The default trainer does not provide a sufficiently reliable baseline without further enhancement.
3. Explore simple interventions including hyperparameter fine-tuning to see whether the baseline can be stabilized.


Li,Bowei:
1. Figure out the code organization and related algorithm implementation.


——————————————————————————————————————————————————————————————————————————————————————————————————————
Plans for Next week:

Jin,Ziyuan:
1. (Cooperate with Li,Bowei) Verify the feasibility of the algorithm. If it is not feasible, replace it with a new algorithm and 
write a demo for verification.


Cui,Hanjia:
1. Keep exploring simple interventions like hyper-parameter fine-tuning.
2. Introduce the basic technique like reward reshaping. Augment the default sparse reward scheme with dense feedback, so agents receive positive reinforcement for each beneficial movement.
3. Assess algorithmic enhancements. Investigate the feasibility of core modifications to improve overall performance.


Li,Bowei:
1. (Cooperate with Jin,Ziyuan) Verify the feasibility of the algorithm. If it is not feasible, replace it with a new algorithm and 
write a demo for verification.

######################################################################################################
third week
time:5.19-5.25
######################################################################################################
This week's progress:

Jin,Ziyuan:
1. Complete the task of last week.
2.Add the reward agent mechanism and complete the code testing.

Cui,Hanjia:
1. Train a model v1 with (1) hyper-parameter fine-tuning; (2) role differentiation (striker v.s. goalie); (3) add a tiny reward to encourage  ball-kicking behavior.
2. Draft and refine prompts to generate reward function.

Li,Bowei:
1. Research on integrating human feedback mechanisms to improve algorithms.


——————————————————————————————————————————————————————————————————————————————————————————————————————
Plans for Next week:

Jin,Ziyuan:
1. Complete the entire project.
2.Test and verify the experimental results.

Cui,Hanjia:
1. Use LLM to generate reward functions. Integrate those functions into our local codebase and validate correctness.
2. Retrain agents with the newly integrated reward functions.
3. Analyze training outcomes to pinpoint strengths and weaknesses in the reward design. Provide targeted feedback to the LLM and update prompts accordingly. Refine subsequent reward-generation prompts and repeat the integration cycle.


Li,Bowei:
1. Integrate human feedback to improve algorithms.


######################################################################################################
fourth week
time:5.26-6.1
######################################################################################################
This week's progress:

Jin,Ziyuan:
1.Switch the model and test using Claude and o3 API.
2.Collect data for local fine-tuning.

Cui,Hanjia:
1. Use LLM to generate reward functions. Integrate those functions into our local codebase and validate correctness.
2. Retrain agents with the newly integrated reward functions.
3. Analyze training outcomes to pinpoint strengths and weaknesses in the reward design. Provide targeted feedback to the LLM and update prompts accordingly. Refine subsequent reward-generation prompts and repeat the integration cycle.


Li,Bowei:



——————————————————————————————————————————————————————————————————————————————————————————————————————
Plans for Next week:

Jin,Ziyuan:
1.Add a local fine-tuning model based on human feedback.
2.Sort out and report the project.

Cui,Hanjia:
1. Summarize the current results, and re-evaluate previously unworked ideas.
2. Prepare the poster and report.


Li,Bowei:

